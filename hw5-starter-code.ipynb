{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7013895,"sourceType":"datasetVersion","datasetId":4032695}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q transformers","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\nimport random\nimport csv\nimport sys\nimport os\n\ndatadir = '/kaggle/input/storycloze-2018'\nsys.path.append(datadir)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class LanguageModel:\n    \n    def __init__(self, model_name='gpt2', device='cpu', mode='greedy', p=0.8):\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        self.model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        self.mode = mode\n        self.p = p\n    \n    def start(self, s):\n        # return a dictionary with keys 'input_ids' and 'attention_masks'. \n        # 'input_ids' should be a tensor of shape [1,n], where n is the number of tokens in the sequence s\n        # tokens can be obtained from self.tokenizer(s)['input_ids']\n        # 'attention_masks' should be a tensor of the same shape [1,n] filled with ones.\n        raise NotImplementedError\n    \n    def step(self, state):\n        # state should be a dictionary with keys 'input_ids' and 'attention_masks' as returned by self.start()\n        # outputs from the LM are obtained by self.model(**state, labels=state['input_ids'])\n        # Logit scores can be obtained from outputs.logits. These last column pertains to the newest prediction. \n        # The probability distribution coming from the last column should be used for the decoding algorithms.\n        # step should return the updated state (new prediction added to state['input_ids'], another 1 added to state['attention_masks'])\n        raise NotImplementedError\n    \n    def ids_to_string(self, ids):\n        # converts the token ids to strings to print out and view\n        return self.tokenizer.decode(ids)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Store both datasets - short_context_data and long_context data. Remember to strip newlines.\n# For each dataset, try each decoder and save the generated text to a file. Include this file in your submission and use it to answer the questions.\n# For the top-p decoder, remember to try different values of p.\n# Use gpt2 with cpu for development, but use gpt2-xl with gpu to answer the questions for the assignment.","metadata":{},"execution_count":null,"outputs":[]}]}